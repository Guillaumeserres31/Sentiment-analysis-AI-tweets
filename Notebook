import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/GenerativeAI tweets.csv'
tweets_df = pd.read_csv(file_path)

# Display the first few rows of the dataframe to understand its structure
tweets_df.head()

####### CLEANING
import re

# Drop missing values
tweets_df_cleaned = tweets_df.dropna()

# Extract only the date component of the 'Datetime' column
# Assuming 'Datetime' is a column in your DataFrame
tweets_df_cleaned['Date'] = pd.to_datetime(tweets_df_cleaned['Datetime']).dt.date

# Check the range of dates
date_range = (tweets_df_cleaned['Date'].min(), tweets_df_cleaned['Date'].max())

# Check unique values in each column
unique_values = {col: tweets_df_cleaned[col].nunique() for col in tweets_df_cleaned.columns}

# Preprocess the 'Text' column to remove links, hashtags, mentions, and unwanted characters
def clean_tweet(tweet):
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet, flags=re.MULTILINE)  # Remove URLs
    tweet = re.sub(r'\@\w+|\#','', tweet)  # Remove mentions and hashtags
    tweet = re.sub(r'[^A-Za-z0-9 ]+', '', tweet)  # Remove special characters
    tweet = tweet.lower()  # Convert to lowercase
    return tweet

tweets_df_cleaned['Cleaned_Text'] = tweets_df_cleaned['Text'].apply(clean_tweet)

# Displaying the cleaned dataset
# tools.display_dataframe_to_user(name="Cleaned Tweets Data", dataframe=tweets_df_cleaned) # Commented out as 'tools' is not defined

# Display the results
date_range, unique_values

##### Analyse
# Import necessary libraries for analysis
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from textblob import TextBlob

# General Trends: Evolution of tweet volume over time
tweets_df_cleaned['Datetime'] = pd.to_datetime(tweets_df_cleaned['Datetime'])
tweets_df_cleaned.set_index('Datetime', inplace=True)
tweet_volume = tweets_df_cleaned.resample('D').size()

# Sentiment Analysis: Adding a sentiment score for each tweet
tweets_df_cleaned['Sentiment'] = tweets_df_cleaned['Cleaned_Text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)
tweets_df_cleaned['Sentiment_Label'] = tweets_df_cleaned['Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))
sentiment_distribution = tweets_df_cleaned['Sentiment_Label'].value_counts()

# Word Cloud: Generating a word cloud for the most frequent terms
text = ' '.join(tweets_df_cleaned['Cleaned_Text'].tolist())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Plotting the results
plt.figure(figsize=(14, 7))

# Plotting tweet volume over time
plt.subplot(1, 3, 1)
tweet_volume.plot()
plt.title('Evolution of Tweet Volume')
plt.xlabel('Date')
plt.ylabel('Number of Tweets')

# Plotting sentiment distribution
plt.subplot(1, 3, 2)
sns.barplot(x=sentiment_distribution.index, y=sentiment_distribution.values)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Plotting word cloud
plt.subplot(1, 3, 3)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Tweets')

plt.tight_layout()
plt.show()

import nltk
nltk.download('punkt') # Download the punkt resource for sentence tokenization
nltk.download('stopwords')
nltk.download('wordnet') # Download the wordnet resource for lemmatization

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import gensim
from gensim import corpora
import re
import random
# Randomly select 20,000 tweets
tweets_df_sampled = tweets_df_cleaned.sample(n=20000, random_state=1)

# Text Preprocessing for LDA
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
docs = tweets_df_sampled['Cleaned_Text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x) if word.isalpha() and word not in stop_words])

# Create a dictionary of words and their frequency
dictionary = corpora.Dictionary(docs)

# Create a document-term matrix
corpus = [dictionary.doc2bow(doc) for doc in docs]

# Topic modeling using LDA
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)

# Print the topics and their top words
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))
    print('\n')

# Word Cloud: Generating a word cloud for the most frequent terms excluding specific words
stopwords = set(['generative', 'ai', 'chatgpt', 'gpt', 'chat', 'generativeai'])
text = ' '.join(tweets_df_cleaned['Cleaned_Text'].tolist())
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(text)

# Plotting word cloud
plt.figure(figsize=(14, 7))
plt.subplot(1, 3, 3)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Tweets')

import pandas as pd
import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Assurez-vous de télécharger les ressources nécessaires de nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
# Lemmatisation des mots et suppression des mots de base
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))  # Liste des mots de base en anglais

def preprocess_text(text):
    words = word_tokenize(text)  # Tokenization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatisation et suppression des mots de base
    return ' '.join(words)

tweets_df_cleaned['Processed_Text'] = tweets_df_cleaned['Cleaned_Text'].apply(preprocess_text)

# Génération du word cloud en excluant les mots spécifiques
exclude_words = set(['generative', 'ai', 'chatgpt', 'gpt', 'chat', 'generativeai'])
final_stopwords = stop_words.union(exclude_words)

text = ' '.join(tweets_df_cleaned['Processed_Text'].tolist())
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=final_stopwords).generate(text)

# Affichage du word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Tweets')
plt.show()

import pandas as pd
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk
from sklearn.feature_extraction.text import CountVectorizer
import plotly.express as px

# Fonction pour obtenir les n-grammes les plus fréquents
def get_top_n_ngrams(corpus, n=None, ngram=2):
    vec = CountVectorizer(ngram_range=(ngram, ngram), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

# Obtenir les 20 bigrammes les plus fréquents
common_bigrams = get_top_n_ngrams(tweets_df_cleaned['Processed_Text'], 20, ngram=2)

# Obtenir les 20 trigrammes les plus fréquents
common_trigrams = get_top_n_ngrams(tweets_df_cleaned['Processed_Text'], 20, ngram=3)

# Les convertir en dataframes
df_bigrams = pd.DataFrame(common_bigrams, columns=['NgramText', 'count'])
df_trigrams = pd.DataFrame(common_trigrams, columns=['NgramText', 'count'])

# Tracer les bigrammes
fig_bigrams = px.bar(df_bigrams[1:], x='NgramText', y='count', title='Bigram Counts', color = 'count', width=1200, height=800)
fig_bigrams.show()

# Tracer les trigrammes
fig_trigrams = px.bar(df_trigrams, x='NgramText', y='count', title='Trigram Counts', color = 'count', width=1200, height=800)
fig_trigrams.show()

import spacy
from spacy.matcher import Matcher

# Charger le modèle anglais de spaCy
nlp = spacy.load('en_core_web_sm')

# Créer un matcher pour détecter les patterns des tâches
matcher = Matcher(nlp.vocab)

# Ajouter des patterns au matcher
patterns = [
    [{"LOWER": "chatgpt"}, {"POS": "VERB"}, {"POS": "NOUN"}],
    [{"LOWER": "gpt"}, {"POS": "VERB"}, {"POS": "NOUN"}],
    [{"LOWER": "ai"}, {"POS": "VERB"}, {"POS": "NOUN"}],
]

matcher.add("TASK_PATTERN", patterns)

# Fonction pour extraire les tâches
def extract_tasks(doc):
    tasks = []
    matches = matcher(doc)
    for match_id, start, end in matches:
        span = doc[start:end]
        tasks.append(span.text)
    return tasks

# Appliquer la fonction sur les tweets
tweets_df_cleaned['Tasks'] = tweets_df_cleaned['Processed_Text'].apply(lambda x: extract_tasks(nlp(x)))

# Filtrer les tweets avec des tâches identifiées
tweets_with_tasks = tweets_df_cleaned[tweets_df_cleaned['Tasks'].str.len() > 0]

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util

# Charger le modèle BERT pour les embeddings de phrases
model = SentenceTransformer('stsb-distilbert-base')

# Exemples de compétences de la base de données ESCO
esco_skills = [
    "Communication",
    "Collaboration",
    "Creativity",
    "Information Skills",
    "Assisting and Caring",
    "Management Skills",
    "Working with Computers",
    "Core Skills and Competences",
    "Thinking Skills and Competences",
    "Self-management Skills and Competences",
    "Social and Communication Skills and Competences",
    "Life Skills and Competences"
]

# Calculer les embeddings pour les compétences
skill_embeddings = model.encode(esco_skills, convert_to_tensor=True)

# Fonction pour trouver les compétences similaires
def find_similar_skills(task):
    task_embedding = model.encode(task, convert_to_tensor=True)
    cos_sim = util.pytorch_cos_sim(task_embedding, skill_embeddings)
    top_matches = cos_sim[0].topk(3)
    similar_skills = [(esco_skills[idx], score.item()) for idx, score in zip(top_matches.indices, top_matches.values)]
    return similar_skills

# Appliquer la fonction sur les tâches identifiées
tweets_with_tasks['Similar_Skills'] = tweets_with_tasks['Tasks'].apply(lambda x: find_similar_skills(' '.join(x)))

# Analyse de sentiment
tweets_with_tasks['Sentiment'] = tweets_with_tasks['Processed_Text'].apply(lambda x: TextBlob(x).sentiment.polarity)
tweets_with_tasks['Sentiment_Label'] = tweets_with_tasks['Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))
sentiment_distribution = tweets_with_tasks['Sentiment_Label'].value_counts()

# Tracer la distribution des sentiments
plt.figure(figsize=(10, 5))
sns.barplot(x=sentiment_distribution.index, y=sentiment_distribution.values)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()
